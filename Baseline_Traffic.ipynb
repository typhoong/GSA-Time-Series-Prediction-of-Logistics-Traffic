{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202cbe03-9a3e-4d6f-9e65-84a2791c71f8",
   "metadata": {},
   "source": [
    "# [모의 경진대회] 교통물류 통행량 시계열 예측 모델\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433b5a2-3f9a-4343-b0d4-36a7507d0f68",
   "metadata": {},
   "source": [
    "## 데이터 디렉토리 구조\n",
    "아래 셀은 실행하지 마세요:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78084cb5-b9dc-470c-8e26-5e9f9d9e60da",
   "metadata": {},
   "source": [
    "DATA/  \n",
    "  \\_train.csv  \n",
    "  \\_validate.csv  \n",
    "  \\_test.csv  \n",
    "  \\_sample_submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a2365f-c14c-4188-b1e8-232ec6ad6c9f",
   "metadata": {},
   "source": [
    "## 필수 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb9697d-407e-43e4-81cb-a5be961bf305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam, SGD\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7204e61f-e8de-44b5-838a-cf0880e42efe",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/1*V_RKPeIxCB9CS_2SsLyKXw.jpeg)\n",
    "\n",
    "EDA(Graph)정리 링크 : https://evening-satin-e96.notion.site/Traffic-EDA-graph-4bafbae26b924d999b8b03f9a055949f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73733c5a-bf4c-4e9c-b4eb-ab98845cfbd4",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20664fd3-632a-4dc1-b062-ecc2483870aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 데이터 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6b2b15f-544f-4476-ad26-5b41ea8ab27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 데이터가 들어있는 폴더 경로 \n",
    "DATASET_PATH = os.path.join('/USER/2nd_competition/Traffic/DATA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dffdec-63ff-49f6-924d-3139e60aa8f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 시간정보를 Index로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d9cc52-2784-46a8-b5e6-60a1927e2fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 경고 무시 \n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('mode.chained_assignment',  None)\n",
    "\n",
    "# 데이터 불러오기\n",
    "train = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "val = pd.read_csv(os.path.join(DATASET_PATH, 'validate.csv'))\n",
    "test = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
    "\n",
    "# 날짜와 시간이 들어있는 columns를 index로 변환하는 함수\n",
    "def datesetting(train): \n",
    "    train['날짜1'] = train['날짜'].astype(str)\n",
    "    train['시간1'] = train['시간'].astype(str)\n",
    "    train['date'] = train['날짜1']+train['시간1']\n",
    "\n",
    "    for i in range (0,len(train['날짜'])):\n",
    "        a = train['날짜1'][i]\n",
    "        train['날짜1'][i] = dt.datetime.strptime(a, '%Y%m%d')\n",
    "\n",
    "    for i in range(0,len(train['시간'])):\n",
    "        b = train['시간1'][i]\n",
    "        train['시간1'][i] = dt.datetime.strptime(b, '%H')\n",
    "\n",
    "    for i in range(0,len(train['날짜'])):\n",
    "        train['date'][i] = train['날짜1'][i]+timedelta(hours=train['시간1'][i].hour)\n",
    "\n",
    "# 날짜와 시간이 들어있는 columns를 index로 변환\n",
    "datesetting(train)\n",
    "datesetting(val)\n",
    "datesetting(test)\n",
    "\n",
    "# 기존의 날짜, 시간 columns 제거\n",
    "train=train.drop(['날짜1','시간1','날짜','시간'],axis=1)\n",
    "val=val.drop(['날짜1','시간1','날짜','시간'],axis=1)\n",
    "test=test.drop(['날짜1','시간1','날짜','시간'],axis=1)\n",
    "\n",
    "# 인덱스 재지정\n",
    "train_df=train.set_index('date')\n",
    "val_df=val.set_index('date')\n",
    "test_df=test.set_index('date')\n",
    "\n",
    "# values를 Float형으로 변환\n",
    "train_df = train_df.astype(float)\n",
    "val_df = val_df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ecefa-d398-4eea-b252-ae2d1b82b086",
   "metadata": {},
   "source": [
    "#### 데이터 shape 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab5c572-b067-414a-a27e-61067fad2c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3279, 35)\n",
      "(336, 35)\n",
      "(336, 35)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape) #train_df의 shape을 확인\n",
    "print(val_df.shape) #val_df의 shape을 확인\n",
    "print(test_df.shape) #test_df의 shape을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b0717-2738-48c9-9236-8aece29e817c",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터 및 기타 인자 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f17a2-1c74-4625-9e26-a6a17ab73a2b",
   "metadata": {},
   "source": [
    "#### 시드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132e347d-b3c3-435e-a266-217748ec3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 생성기가 항상 일정한 값을 출력하게 하기 위해 seed 고정\n",
    "random_seed = 2022\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb72c7-6bc1-4ea2-bc10-b5eb6fb8091c",
   "metadata": {},
   "source": [
    "#### 디바이스 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11620428-187e-4e7b-b2cd-33f853b054cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e76c9-b188-4521-b433-b0d6414d13c2",
   "metadata": {},
   "source": [
    "#### 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ec8765-b425-4b07-b2e4-c8cb1cf95fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 1                                   # Dataset 길이에 맞추어 1로 설정                           \n",
    "FEATURE_DIM = 35                                   #feature dim -> 35개 도로입니다.\n",
    "BACKCAST_LENGTH = 168\n",
    "FORECAST_LENGTH = 168"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd8673-ab6e-4af3-9215-17c2f9014932",
   "metadata": {},
   "source": [
    "#### 기타 인자 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eecd823-87f0-4ada-96a0-48064a1912d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIN_MEMORY = True     # True로 한 뒤에 작동이 느리거나 이상이 있다면 False, default 값은 False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7eba3b-244e-4033-9e1b-f579a8182e76",
   "metadata": {},
   "source": [
    "## Dataset 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f00670-f4de-4e1d-a87f-6f42a8a43e73",
   "metadata": {},
   "source": [
    "#### Scaler 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075d2bb6-0b28-40fc-b7d7-325101d09f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 normailze하는 클래스\n",
    "class StandardScaler():   \n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.stdev = None\n",
    "        \n",
    "    def fit(self, x, dim=None):\n",
    "        if not torch.is_tensor(x):  # numpy array -> torch tensor \n",
    "            x = torch.tensor(x)\n",
    "\n",
    "        \n",
    "        if dim is not None:\n",
    "            self.mean = x.mean(dim)\n",
    "            self.stdev = x.std(dim)\n",
    "        else:\n",
    "            self.mean = x.mean()\n",
    "            self.stdev = x.std()\n",
    "        \n",
    "    def scale(self, x):\n",
    "\n",
    "        if not torch.is_tensor(x): # numpy array -> torch tensor \n",
    "            x = torch.tensor(x)\n",
    "        return (x - self.mean.to(x.device)) / self.stdev.to(x.device)\n",
    "    \n",
    "    def unscale(self, x):\n",
    "        if not torch.is_tensor(x): # numpy array -> torch tensor \n",
    "            x = torch.tensor(x)\n",
    "        return x * self.stdev.to(x.device) + self.mean.to(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760a38-43f2-4d14-a732-2e8779707bec",
   "metadata": {},
   "source": [
    "#### Custom Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba38fca-c538-49fc-96bc-5571ec32b618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeseriesDataset(torch.utils.data.Dataset):                             \n",
    "    def __init__(self, data, backcast_length, forecast_length, scaler=None): \n",
    "        self.data_columns = data.columns                                       #columns 지정\n",
    "        self.data = data.values                                                #values 지정\n",
    "        self.backcast_length = backcast_length                                 #input data 길이 지정\n",
    "        self.forecast_length = forecast_length                                 #output data 길이 지정\n",
    "        \n",
    "        if scaler is None:\n",
    "            # train\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(self.data, dim=0)\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "        self.data = self.scaler.scale(self.data)\n",
    "        \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.data) - self.backcast_length - self.forecast_length + 1\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        x_end = idx + self.backcast_length\n",
    "        y_end = x_end + self.forecast_length\n",
    "\n",
    "        x = self.data[idx:x_end]\n",
    "        y = self.data[x_end:y_end]\n",
    "        \n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        \n",
    "        return x, y \n",
    "    \n",
    "    def unscale(self, x):\n",
    "        return self.scaler.unscale(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3438922-77dc-4093-b1b7-28fe39da9e3a",
   "metadata": {},
   "source": [
    "# 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad3104e7-cdb2-4dfd-8dd9-595a9c347a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_dim, LSTM=False, GRU=True, num_layers=4, \n",
    "                 bidirectional=False, dropout=0.3, batch_first=True, \n",
    "                 backcast_length=168, forecast_length=168):\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.LSTM = LSTM # LSTM인지 아닌지 여부를 저장\n",
    "        self.n_direction = 2 if bidirectional else 1\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        \n",
    "        if LSTM:\n",
    "            rnn = nn.LSTM\n",
    "        elif GRU:\n",
    "            rnn = nn.GRU\n",
    "        else:\n",
    "            rnn = nn.RNN\n",
    "            \n",
    "            \n",
    "        self.rnn = rnn(input_size = feature_dim, \n",
    "                       hidden_size = hidden_dim, \n",
    "                       num_layers = num_layers,\n",
    "                       bias = True,\n",
    "                       batch_first = batch_first,\n",
    "                       dropout = dropout,\n",
    "                       bidirectional = bidirectional)\n",
    "\n",
    "        self.out = nn.Linear(hidden_dim * self.n_direction, feature_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.LSTM:\n",
    "            h0 = (torch.zeros(self.num_layers * self.n_direction, x.shape[0], self.hidden_dim, device=x.device),\n",
    "                  torch.zeros(self.num_layers * self.n_direction, x.shape[0], self.hidden_dim, device=x.device))\n",
    "        else:\n",
    "            h0 = torch.zeros(self.num_layers * self.n_direction, x.shape[0], self.hidden_dim, device=x.device)\n",
    "        \n",
    "        x, h = self.rnn(x, h0)\n",
    "        outs = []\n",
    "        for i in range(self.forecast_length):\n",
    "            x = self.out(x[:,-1:,:])\n",
    "            outs.append(x.squeeze(0))\n",
    "            \n",
    "            if i == self.forecast_length-1:\n",
    "                break\n",
    "                \n",
    "            x, h = self.rnn(x, h)\n",
    "            \n",
    "        outs = torch.stack(outs, dim=1)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc421b74-31be-48e0-bf4a-33e9b1ac8ce6",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af11a66-8384-4794-808b-64dd63ddbb09",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dataset & Dataloader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c1e4ec6-ce70-45df-a747-d63d449875f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TimeseriesDataset(train_df, BACKCAST_LENGTH, FORECAST_LENGTH, scaler=None)\n",
    "val_dataset = TimeseriesDataset(val_df, BACKCAST_LENGTH, FORECAST_LENGTH, scaler=train_dataset.scaler)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                               num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                               num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a47f3d-3427-4f9c-899f-e3f9c441f48a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 모델과 기타 인자 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77153161-5c3a-40bf-983d-c2aae44db7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = RNNmodel(FEATURE_DIM, HIDDEN_DIM, LSTM=False, GRU=True).to(DEVICE) #성능고도화를 위해 GRU = True로 설정해서 진행\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f6f2c-3c6f-404e-a050-60453694a081",
   "metadata": {},
   "source": [
    "#### Epoch 단위 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3833a212-51bb-4d19-9a6f-0dc5eb803fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [0/20] iter [000/2944] loss [1.0530] elapsed time [0.01min]\n",
      "epoch [0/20] iter [100/2944] loss [0.8678] elapsed time [0.42min]\n",
      "epoch [0/20] iter [200/2944] loss [0.6712] elapsed time [0.81min]\n",
      "epoch [0/20] iter [300/2944] loss [0.4550] elapsed time [1.22min]\n",
      "epoch [0/20] iter [400/2944] loss [0.2842] elapsed time [1.62min]\n",
      "epoch [0/20] iter [500/2944] loss [1.1019] elapsed time [2.04min]\n",
      "epoch [0/20] iter [600/2944] loss [0.1860] elapsed time [2.44min]\n",
      "epoch [0/20] iter [700/2944] loss [0.1632] elapsed time [2.85min]\n",
      "epoch [0/20] iter [800/2944] loss [0.4152] elapsed time [3.27min]\n",
      "epoch [0/20] iter [900/2944] loss [0.1604] elapsed time [3.68min]\n",
      "epoch [0/20] iter [1000/2944] loss [0.9735] elapsed time [4.09min]\n",
      "epoch [0/20] iter [1100/2944] loss [0.2063] elapsed time [4.51min]\n",
      "epoch [0/20] iter [1200/2944] loss [0.2767] elapsed time [4.92min]\n",
      "epoch [0/20] iter [1300/2944] loss [0.2834] elapsed time [5.33min]\n",
      "epoch [0/20] iter [1400/2944] loss [0.2206] elapsed time [5.72min]\n",
      "epoch [0/20] iter [1500/2944] loss [0.2598] elapsed time [6.13min]\n",
      "epoch [0/20] iter [1600/2944] loss [0.1543] elapsed time [6.53min]\n",
      "epoch [0/20] iter [1700/2944] loss [0.5656] elapsed time [6.89min]\n",
      "epoch [0/20] iter [1800/2944] loss [0.1725] elapsed time [7.24min]\n",
      "epoch [0/20] iter [1900/2944] loss [0.1807] elapsed time [7.64min]\n",
      "epoch [0/20] iter [2000/2944] loss [0.1947] elapsed time [8.05min]\n",
      "epoch [0/20] iter [2100/2944] loss [0.1660] elapsed time [8.47min]\n",
      "epoch [0/20] iter [2200/2944] loss [0.1858] elapsed time [8.88min]\n",
      "epoch [0/20] iter [2300/2944] loss [0.1799] elapsed time [9.29min]\n",
      "epoch [0/20] iter [2400/2944] loss [0.1757] elapsed time [9.70min]\n",
      "epoch [0/20] iter [2500/2944] loss [0.2278] elapsed time [10.11min]\n",
      "epoch [0/20] iter [2600/2944] loss [0.2758] elapsed time [10.52min]\n",
      "epoch [0/20] iter [2700/2944] loss [0.1667] elapsed time [10.92min]\n",
      "epoch [0/20] iter [2800/2944] loss [0.3403] elapsed time [11.32min]\n",
      "epoch [0/20] iter [2900/2944] loss [0.2272] elapsed time [11.74min]\n",
      "epoch [0/20] train loss [0.4577] validation loss [0.2606] elapsed time [11.92 min]\n",
      "\n",
      "epoch [1/20] iter [000/2944] loss [0.5707] elapsed time [11.93min]\n",
      "epoch [1/20] iter [100/2944] loss [0.1423] elapsed time [12.33min]\n",
      "epoch [1/20] iter [200/2944] loss [0.2683] elapsed time [12.72min]\n",
      "epoch [1/20] iter [300/2944] loss [0.1440] elapsed time [13.13min]\n",
      "epoch [1/20] iter [400/2944] loss [0.1804] elapsed time [13.52min]\n",
      "epoch [1/20] iter [500/2944] loss [0.1366] elapsed time [13.91min]\n",
      "epoch [1/20] iter [600/2944] loss [0.1815] elapsed time [14.31min]\n",
      "epoch [1/20] iter [700/2944] loss [1.2879] elapsed time [14.70min]\n",
      "epoch [1/20] iter [800/2944] loss [0.2026] elapsed time [15.09min]\n",
      "epoch [1/20] iter [900/2944] loss [0.2982] elapsed time [15.49min]\n",
      "epoch [1/20] iter [1000/2944] loss [1.0424] elapsed time [15.88min]\n",
      "epoch [1/20] iter [1100/2944] loss [1.0592] elapsed time [16.27min]\n",
      "epoch [1/20] iter [1200/2944] loss [0.1526] elapsed time [16.67min]\n",
      "epoch [1/20] iter [1300/2944] loss [0.3114] elapsed time [17.05min]\n",
      "epoch [1/20] iter [1400/2944] loss [1.2017] elapsed time [17.44min]\n",
      "epoch [1/20] iter [1500/2944] loss [0.2391] elapsed time [17.83min]\n",
      "epoch [1/20] iter [1600/2944] loss [0.1829] elapsed time [18.21min]\n",
      "epoch [1/20] iter [1700/2944] loss [0.2639] elapsed time [18.59min]\n",
      "epoch [1/20] iter [1800/2944] loss [0.1888] elapsed time [18.98min]\n",
      "epoch [1/20] iter [1900/2944] loss [0.1375] elapsed time [19.36min]\n",
      "epoch [1/20] iter [2000/2944] loss [0.1622] elapsed time [19.75min]\n",
      "epoch [1/20] iter [2100/2944] loss [0.2967] elapsed time [20.13min]\n",
      "epoch [1/20] iter [2200/2944] loss [0.2463] elapsed time [20.51min]\n",
      "epoch [1/20] iter [2300/2944] loss [0.1440] elapsed time [20.90min]\n",
      "epoch [1/20] iter [2400/2944] loss [0.5030] elapsed time [21.29min]\n",
      "epoch [1/20] iter [2500/2944] loss [0.3261] elapsed time [21.68min]\n",
      "epoch [1/20] iter [2600/2944] loss [0.1414] elapsed time [22.06min]\n",
      "epoch [1/20] iter [2700/2944] loss [0.2167] elapsed time [22.45min]\n",
      "epoch [1/20] iter [2800/2944] loss [0.1942] elapsed time [22.84min]\n",
      "epoch [1/20] iter [2900/2944] loss [0.1371] elapsed time [23.23min]\n",
      "epoch [1/20] train loss [0.3648] validation loss [0.2339] elapsed time [23.41 min]\n",
      "\n",
      "epoch [2/20] iter [000/2944] loss [0.1403] elapsed time [23.41min]\n",
      "epoch [2/20] iter [100/2944] loss [0.1668] elapsed time [23.82min]\n",
      "epoch [2/20] iter [200/2944] loss [0.1587] elapsed time [24.22min]\n",
      "epoch [2/20] iter [300/2944] loss [0.6988] elapsed time [24.62min]\n",
      "epoch [2/20] iter [400/2944] loss [0.1805] elapsed time [25.01min]\n",
      "epoch [2/20] iter [500/2944] loss [0.1428] elapsed time [25.43min]\n",
      "epoch [2/20] iter [600/2944] loss [0.1751] elapsed time [25.83min]\n",
      "epoch [2/20] iter [700/2944] loss [0.1531] elapsed time [26.24min]\n",
      "epoch [2/20] iter [800/2944] loss [0.1344] elapsed time [26.65min]\n",
      "epoch [2/20] iter [900/2944] loss [0.1490] elapsed time [27.06min]\n",
      "epoch [2/20] iter [1000/2944] loss [0.2313] elapsed time [27.46min]\n",
      "epoch [2/20] iter [1100/2944] loss [0.2601] elapsed time [27.87min]\n",
      "epoch [2/20] iter [1200/2944] loss [0.1820] elapsed time [28.27min]\n",
      "epoch [2/20] iter [1300/2944] loss [0.1729] elapsed time [28.67min]\n",
      "epoch [2/20] iter [1400/2944] loss [0.3501] elapsed time [29.06min]\n",
      "epoch [2/20] iter [1500/2944] loss [1.0806] elapsed time [29.46min]\n",
      "epoch [2/20] iter [1600/2944] loss [0.1831] elapsed time [29.87min]\n",
      "epoch [2/20] iter [1700/2944] loss [0.1860] elapsed time [30.29min]\n",
      "epoch [2/20] iter [1800/2944] loss [1.7231] elapsed time [30.65min]\n",
      "epoch [2/20] iter [1900/2944] loss [0.1721] elapsed time [31.03min]\n",
      "epoch [2/20] iter [2000/2944] loss [0.6771] elapsed time [31.43min]\n",
      "epoch [2/20] iter [2100/2944] loss [0.1843] elapsed time [31.79min]\n",
      "epoch [2/20] iter [2200/2944] loss [0.1266] elapsed time [32.15min]\n",
      "epoch [2/20] iter [2300/2944] loss [0.4876] elapsed time [32.51min]\n",
      "epoch [2/20] iter [2400/2944] loss [0.1439] elapsed time [32.87min]\n",
      "epoch [2/20] iter [2500/2944] loss [0.2014] elapsed time [33.24min]\n",
      "epoch [2/20] iter [2600/2944] loss [0.2142] elapsed time [33.59min]\n",
      "epoch [2/20] iter [2700/2944] loss [1.0686] elapsed time [33.95min]\n",
      "epoch [2/20] iter [2800/2944] loss [0.9773] elapsed time [34.31min]\n",
      "epoch [2/20] iter [2900/2944] loss [1.7697] elapsed time [34.67min]\n",
      "epoch [2/20] train loss [0.3507] validation loss [0.2449] elapsed time [34.83 min]\n",
      "\n",
      "epoch [3/20] iter [000/2944] loss [0.5259] elapsed time [34.84min]\n",
      "epoch [3/20] iter [100/2944] loss [0.1276] elapsed time [35.21min]\n",
      "epoch [3/20] iter [200/2944] loss [0.1276] elapsed time [35.58min]\n",
      "epoch [3/20] iter [300/2944] loss [0.1076] elapsed time [35.96min]\n",
      "epoch [3/20] iter [400/2944] loss [0.1202] elapsed time [36.33min]\n",
      "epoch [3/20] iter [500/2944] loss [0.1886] elapsed time [36.71min]\n",
      "epoch [3/20] iter [600/2944] loss [0.1706] elapsed time [37.07min]\n",
      "epoch [3/20] iter [700/2944] loss [0.1731] elapsed time [37.44min]\n",
      "epoch [3/20] iter [800/2944] loss [0.2376] elapsed time [37.83min]\n",
      "epoch [3/20] iter [900/2944] loss [0.1331] elapsed time [38.19min]\n",
      "epoch [3/20] iter [1000/2944] loss [0.4017] elapsed time [38.58min]\n",
      "epoch [3/20] iter [1100/2944] loss [1.3084] elapsed time [38.94min]\n",
      "epoch [3/20] iter [1200/2944] loss [0.1516] elapsed time [39.30min]\n",
      "epoch [3/20] iter [1300/2944] loss [0.1430] elapsed time [39.66min]\n",
      "epoch [3/20] iter [1400/2944] loss [0.1670] elapsed time [40.03min]\n",
      "epoch [3/20] iter [1500/2944] loss [1.0816] elapsed time [40.39min]\n",
      "epoch [3/20] iter [1600/2944] loss [0.2058] elapsed time [40.75min]\n",
      "epoch [3/20] iter [1700/2944] loss [0.5497] elapsed time [41.11min]\n",
      "epoch [3/20] iter [1800/2944] loss [0.1533] elapsed time [41.47min]\n",
      "epoch [3/20] iter [1900/2944] loss [1.1223] elapsed time [41.83min]\n",
      "epoch [3/20] iter [2000/2944] loss [0.1572] elapsed time [42.19min]\n",
      "epoch [3/20] iter [2100/2944] loss [0.1309] elapsed time [42.55min]\n",
      "epoch [3/20] iter [2200/2944] loss [0.9532] elapsed time [42.91min]\n",
      "epoch [3/20] iter [2300/2944] loss [0.1498] elapsed time [43.27min]\n",
      "epoch [3/20] iter [2400/2944] loss [0.2540] elapsed time [43.63min]\n",
      "epoch [3/20] iter [2500/2944] loss [0.1490] elapsed time [43.99min]\n",
      "epoch [3/20] iter [2600/2944] loss [0.1623] elapsed time [44.36min]\n",
      "epoch [3/20] iter [2700/2944] loss [0.3335] elapsed time [44.72min]\n",
      "epoch [3/20] iter [2800/2944] loss [0.1321] elapsed time [45.08min]\n",
      "epoch [3/20] iter [2900/2944] loss [0.1781] elapsed time [45.44min]\n",
      "epoch [3/20] train loss [0.3461] validation loss [0.2358] elapsed time [45.60 min]\n",
      "\n",
      "epoch [4/20] iter [000/2944] loss [0.1629] elapsed time [45.61min]\n",
      "epoch [4/20] iter [100/2944] loss [0.1252] elapsed time [45.99min]\n",
      "epoch [4/20] iter [200/2944] loss [0.1678] elapsed time [46.36min]\n",
      "epoch [4/20] iter [300/2944] loss [0.1377] elapsed time [46.75min]\n",
      "epoch [4/20] iter [400/2944] loss [0.1967] elapsed time [47.11min]\n",
      "epoch [4/20] iter [500/2944] loss [0.3661] elapsed time [47.49min]\n",
      "epoch [4/20] iter [600/2944] loss [0.1458] elapsed time [47.88min]\n",
      "epoch [4/20] iter [700/2944] loss [2.1452] elapsed time [48.28min]\n",
      "epoch [4/20] iter [800/2944] loss [0.1389] elapsed time [48.67min]\n",
      "epoch [4/20] iter [900/2944] loss [1.0753] elapsed time [49.05min]\n",
      "epoch [4/20] iter [1000/2944] loss [0.1181] elapsed time [49.44min]\n",
      "epoch [4/20] iter [1100/2944] loss [0.1478] elapsed time [49.84min]\n",
      "epoch [4/20] iter [1200/2944] loss [0.1410] elapsed time [50.20min]\n",
      "epoch [4/20] iter [1300/2944] loss [0.2397] elapsed time [50.58min]\n",
      "epoch [4/20] iter [1400/2944] loss [0.4252] elapsed time [50.98min]\n",
      "epoch [4/20] iter [1500/2944] loss [0.1192] elapsed time [51.38min]\n",
      "epoch [4/20] iter [1600/2944] loss [0.7254] elapsed time [51.79min]\n",
      "epoch [4/20] iter [1700/2944] loss [0.1829] elapsed time [52.20min]\n",
      "epoch [4/20] iter [1800/2944] loss [0.1309] elapsed time [52.60min]\n",
      "epoch [4/20] iter [1900/2944] loss [0.2951] elapsed time [53.01min]\n",
      "epoch [4/20] iter [2000/2944] loss [0.9301] elapsed time [53.41min]\n",
      "epoch [4/20] iter [2100/2944] loss [0.1303] elapsed time [53.81min]\n",
      "epoch [4/20] iter [2200/2944] loss [0.1245] elapsed time [54.21min]\n",
      "epoch [4/20] iter [2300/2944] loss [0.2203] elapsed time [54.61min]\n",
      "epoch [4/20] iter [2400/2944] loss [0.1304] elapsed time [55.01min]\n",
      "epoch [4/20] iter [2500/2944] loss [0.2052] elapsed time [55.42min]\n",
      "epoch [4/20] iter [2600/2944] loss [0.1309] elapsed time [55.82min]\n",
      "epoch [4/20] iter [2700/2944] loss [0.2099] elapsed time [56.22min]\n",
      "epoch [4/20] iter [2800/2944] loss [0.1152] elapsed time [56.61min]\n",
      "epoch [4/20] iter [2900/2944] loss [0.1616] elapsed time [56.99min]\n",
      "epoch [4/20] train loss [0.3384] validation loss [0.4219] elapsed time [57.17 min]\n",
      "\n",
      "epoch [5/20] iter [000/2944] loss [0.3948] elapsed time [57.18min]\n",
      "epoch [5/20] iter [100/2944] loss [0.1654] elapsed time [57.58min]\n",
      "epoch [5/20] iter [200/2944] loss [0.1604] elapsed time [57.97min]\n",
      "epoch [5/20] iter [300/2944] loss [0.2536] elapsed time [58.38min]\n",
      "epoch [5/20] iter [400/2944] loss [0.2014] elapsed time [58.79min]\n",
      "epoch [5/20] iter [500/2944] loss [0.1591] elapsed time [59.21min]\n",
      "epoch [5/20] iter [600/2944] loss [0.1999] elapsed time [59.61min]\n",
      "epoch [5/20] iter [700/2944] loss [0.1803] elapsed time [60.02min]\n",
      "epoch [5/20] iter [800/2944] loss [0.1249] elapsed time [60.43min]\n",
      "epoch [5/20] iter [900/2944] loss [0.4645] elapsed time [60.84min]\n",
      "epoch [5/20] iter [1000/2944] loss [0.1625] elapsed time [61.25min]\n",
      "epoch [5/20] iter [1100/2944] loss [0.1759] elapsed time [61.66min]\n",
      "epoch [5/20] iter [1200/2944] loss [0.1281] elapsed time [62.08min]\n",
      "epoch [5/20] iter [1300/2944] loss [0.1175] elapsed time [62.49min]\n",
      "epoch [5/20] iter [1400/2944] loss [0.1817] elapsed time [62.91min]\n",
      "epoch [5/20] iter [1500/2944] loss [0.1397] elapsed time [63.32min]\n",
      "epoch [5/20] iter [1600/2944] loss [0.4781] elapsed time [63.74min]\n",
      "epoch [5/20] iter [1700/2944] loss [0.1367] elapsed time [64.15min]\n",
      "epoch [5/20] iter [1800/2944] loss [0.2163] elapsed time [64.54min]\n",
      "epoch [5/20] iter [1900/2944] loss [0.1688] elapsed time [64.96min]\n",
      "epoch [5/20] iter [2000/2944] loss [2.3073] elapsed time [65.37min]\n",
      "epoch [5/20] iter [2100/2944] loss [0.2274] elapsed time [65.75min]\n",
      "epoch [5/20] iter [2200/2944] loss [1.1582] elapsed time [66.11min]\n",
      "epoch [5/20] iter [2300/2944] loss [0.1466] elapsed time [66.49min]\n",
      "epoch [5/20] iter [2400/2944] loss [1.0929] elapsed time [66.86min]\n",
      "epoch [5/20] iter [2500/2944] loss [0.1528] elapsed time [67.22min]\n",
      "epoch [5/20] iter [2600/2944] loss [0.3073] elapsed time [67.58min]\n",
      "epoch [5/20] iter [2700/2944] loss [0.1331] elapsed time [67.94min]\n",
      "epoch [5/20] iter [2800/2944] loss [0.0988] elapsed time [68.30min]\n",
      "epoch [5/20] iter [2900/2944] loss [0.1207] elapsed time [68.66min]\n",
      "epoch [5/20] train loss [0.3343] validation loss [0.2157] elapsed time [68.82 min]\n",
      "\n",
      "epoch [6/20] iter [000/2944] loss [0.1570] elapsed time [68.82min]\n",
      "epoch [6/20] iter [100/2944] loss [0.1170] elapsed time [69.19min]\n",
      "epoch [6/20] iter [200/2944] loss [0.8777] elapsed time [69.56min]\n",
      "epoch [6/20] iter [300/2944] loss [0.1287] elapsed time [69.92min]\n",
      "epoch [6/20] iter [400/2944] loss [0.1612] elapsed time [70.29min]\n",
      "epoch [6/20] iter [500/2944] loss [0.1425] elapsed time [70.66min]\n",
      "epoch [6/20] iter [600/2944] loss [0.1737] elapsed time [71.02min]\n",
      "epoch [6/20] iter [700/2944] loss [0.1585] elapsed time [71.40min]\n",
      "epoch [6/20] iter [800/2944] loss [0.8853] elapsed time [71.78min]\n",
      "epoch [6/20] iter [900/2944] loss [0.1400] elapsed time [72.15min]\n",
      "epoch [6/20] iter [1000/2944] loss [0.1133] elapsed time [72.51min]\n",
      "epoch [6/20] iter [1100/2944] loss [0.1537] elapsed time [72.88min]\n",
      "epoch [6/20] iter [1200/2944] loss [0.1626] elapsed time [73.27min]\n",
      "epoch [6/20] iter [1300/2944] loss [0.1356] elapsed time [73.63min]\n",
      "epoch [6/20] iter [1400/2944] loss [0.1850] elapsed time [73.99min]\n",
      "epoch [6/20] iter [1500/2944] loss [1.0480] elapsed time [74.35min]\n",
      "epoch [6/20] iter [1600/2944] loss [0.1326] elapsed time [74.71min]\n",
      "epoch [6/20] iter [1700/2944] loss [0.1551] elapsed time [75.07min]\n",
      "epoch [6/20] iter [1800/2944] loss [0.3279] elapsed time [75.45min]\n",
      "epoch [6/20] iter [1900/2944] loss [0.2199] elapsed time [75.83min]\n",
      "epoch [6/20] iter [2000/2944] loss [0.1750] elapsed time [76.20min]\n",
      "epoch [6/20] iter [2100/2944] loss [1.0941] elapsed time [76.59min]\n",
      "epoch [6/20] iter [2200/2944] loss [0.1455] elapsed time [76.97min]\n",
      "epoch [6/20] iter [2300/2944] loss [0.1169] elapsed time [77.35min]\n",
      "epoch [6/20] iter [2400/2944] loss [0.1643] elapsed time [77.72min]\n",
      "epoch [6/20] iter [2500/2944] loss [0.1334] elapsed time [78.10min]\n",
      "epoch [6/20] iter [2600/2944] loss [0.1306] elapsed time [78.48min]\n",
      "epoch [6/20] iter [2700/2944] loss [0.1565] elapsed time [78.86min]\n",
      "epoch [6/20] iter [2800/2944] loss [0.1379] elapsed time [79.23min]\n",
      "epoch [6/20] iter [2900/2944] loss [0.1467] elapsed time [79.60min]\n",
      "epoch [6/20] train loss [0.3317] validation loss [0.2163] elapsed time [79.77 min]\n",
      "\n",
      "epoch [7/20] iter [000/2944] loss [0.2086] elapsed time [79.78min]\n",
      "epoch [7/20] iter [100/2944] loss [0.1238] elapsed time [80.16min]\n",
      "epoch [7/20] iter [200/2944] loss [0.1449] elapsed time [80.56min]\n",
      "epoch [7/20] iter [300/2944] loss [0.2228] elapsed time [80.96min]\n",
      "epoch [7/20] iter [400/2944] loss [0.7130] elapsed time [81.37min]\n",
      "epoch [7/20] iter [500/2944] loss [0.1489] elapsed time [81.77min]\n",
      "epoch [7/20] iter [600/2944] loss [0.1404] elapsed time [82.17min]\n",
      "epoch [7/20] iter [700/2944] loss [0.1829] elapsed time [82.58min]\n",
      "epoch [7/20] iter [800/2944] loss [1.0938] elapsed time [82.98min]\n",
      "epoch [7/20] iter [900/2944] loss [0.1490] elapsed time [83.38min]\n",
      "epoch [7/20] iter [1000/2944] loss [0.3479] elapsed time [83.77min]\n",
      "epoch [7/20] iter [1100/2944] loss [0.3359] elapsed time [84.17min]\n",
      "epoch [7/20] iter [1200/2944] loss [0.1651] elapsed time [84.55min]\n",
      "epoch [7/20] iter [1300/2944] loss [0.2743] elapsed time [84.94min]\n",
      "epoch [7/20] iter [1400/2944] loss [0.4113] elapsed time [85.34min]\n",
      "epoch [7/20] iter [1500/2944] loss [0.1267] elapsed time [85.74min]\n",
      "epoch [7/20] iter [1600/2944] loss [0.3915] elapsed time [86.12min]\n",
      "epoch [7/20] iter [1700/2944] loss [0.1478] elapsed time [86.50min]\n",
      "epoch [7/20] iter [1800/2944] loss [0.1637] elapsed time [86.89min]\n",
      "epoch [7/20] iter [1900/2944] loss [0.1657] elapsed time [87.27min]\n",
      "epoch [7/20] iter [2000/2944] loss [0.1671] elapsed time [87.65min]\n",
      "epoch [7/20] iter [2100/2944] loss [0.2822] elapsed time [88.04min]\n",
      "epoch [7/20] iter [2200/2944] loss [0.2176] elapsed time [88.42min]\n",
      "epoch [7/20] iter [2300/2944] loss [0.1228] elapsed time [88.80min]\n",
      "epoch [7/20] iter [2400/2944] loss [0.1377] elapsed time [89.18min]\n",
      "epoch [7/20] iter [2500/2944] loss [0.7087] elapsed time [89.56min]\n",
      "epoch [7/20] iter [2600/2944] loss [0.6047] elapsed time [89.95min]\n",
      "epoch [7/20] iter [2700/2944] loss [0.1598] elapsed time [90.33min]\n",
      "epoch [7/20] iter [2800/2944] loss [0.2631] elapsed time [90.72min]\n",
      "epoch [7/20] iter [2900/2944] loss [0.1291] elapsed time [91.10min]\n",
      "epoch [7/20] train loss [0.3308] validation loss [0.2194] elapsed time [91.27 min]\n",
      "\n",
      "epoch [8/20] iter [000/2944] loss [0.2666] elapsed time [91.28min]\n",
      "epoch [8/20] iter [100/2944] loss [0.1715] elapsed time [91.69min]\n",
      "epoch [8/20] iter [200/2944] loss [0.1290] elapsed time [92.11min]\n",
      "epoch [8/20] iter [300/2944] loss [0.3248] elapsed time [92.52min]\n",
      "epoch [8/20] iter [400/2944] loss [0.1544] elapsed time [92.91min]\n",
      "epoch [8/20] iter [500/2944] loss [0.2441] elapsed time [93.28min]\n",
      "epoch [8/20] iter [600/2944] loss [0.2627] elapsed time [93.65min]\n",
      "epoch [8/20] iter [700/2944] loss [0.1294] elapsed time [94.01min]\n",
      "epoch [8/20] iter [800/2944] loss [0.1356] elapsed time [94.38min]\n",
      "epoch [8/20] iter [900/2944] loss [0.1427] elapsed time [94.76min]\n",
      "epoch [8/20] iter [1000/2944] loss [0.1616] elapsed time [95.14min]\n",
      "epoch [8/20] iter [1100/2944] loss [1.5160] elapsed time [95.51min]\n",
      "epoch [8/20] iter [1200/2944] loss [0.1554] elapsed time [95.89min]\n",
      "epoch [8/20] iter [1300/2944] loss [0.1799] elapsed time [96.26min]\n",
      "epoch [8/20] iter [1400/2944] loss [0.9683] elapsed time [96.62min]\n",
      "epoch [8/20] iter [1500/2944] loss [0.1215] elapsed time [96.99min]\n",
      "epoch [8/20] iter [1600/2944] loss [0.1917] elapsed time [97.35min]\n",
      "epoch [8/20] iter [1700/2944] loss [0.1137] elapsed time [97.73min]\n",
      "epoch [8/20] iter [1800/2944] loss [0.1359] elapsed time [98.10min]\n",
      "epoch [8/20] iter [1900/2944] loss [0.1732] elapsed time [98.47min]\n",
      "epoch [8/20] iter [2000/2944] loss [0.1165] elapsed time [98.83min]\n",
      "epoch [8/20] iter [2100/2944] loss [0.1226] elapsed time [99.19min]\n",
      "epoch [8/20] iter [2200/2944] loss [0.1538] elapsed time [99.55min]\n",
      "epoch [8/20] iter [2300/2944] loss [0.1658] elapsed time [99.91min]\n",
      "epoch [8/20] iter [2400/2944] loss [0.1850] elapsed time [100.27min]\n",
      "epoch [8/20] iter [2500/2944] loss [0.1487] elapsed time [100.63min]\n",
      "epoch [8/20] iter [2600/2944] loss [0.1374] elapsed time [100.99min]\n",
      "epoch [8/20] iter [2700/2944] loss [0.1195] elapsed time [101.35min]\n",
      "epoch [8/20] iter [2800/2944] loss [0.8330] elapsed time [101.71min]\n",
      "epoch [8/20] iter [2900/2944] loss [0.3602] elapsed time [102.07min]\n",
      "epoch [8/20] train loss [0.3284] validation loss [0.2206] elapsed time [102.24 min]\n",
      "\n",
      "epoch [9/20] iter [000/2944] loss [0.1263] elapsed time [102.25min]\n",
      "epoch [9/20] iter [100/2944] loss [0.1761] elapsed time [102.61min]\n",
      "epoch [9/20] iter [200/2944] loss [0.1299] elapsed time [102.97min]\n",
      "epoch [9/20] iter [300/2944] loss [0.1607] elapsed time [103.33min]\n",
      "epoch [9/20] iter [400/2944] loss [0.2003] elapsed time [103.69min]\n",
      "epoch [9/20] iter [500/2944] loss [0.1668] elapsed time [104.05min]\n",
      "epoch [9/20] iter [600/2944] loss [0.1432] elapsed time [104.41min]\n",
      "epoch [9/20] iter [700/2944] loss [0.1471] elapsed time [104.77min]\n",
      "epoch [9/20] iter [800/2944] loss [0.1822] elapsed time [105.13min]\n",
      "epoch [9/20] iter [900/2944] loss [0.1143] elapsed time [105.49min]\n",
      "epoch [9/20] iter [1000/2944] loss [0.1221] elapsed time [105.85min]\n",
      "epoch [9/20] iter [1100/2944] loss [0.1083] elapsed time [106.21min]\n",
      "epoch [9/20] iter [1200/2944] loss [0.1437] elapsed time [106.57min]\n",
      "epoch [9/20] iter [1300/2944] loss [0.1804] elapsed time [106.93min]\n",
      "epoch [9/20] iter [1400/2944] loss [0.1957] elapsed time [107.28min]\n",
      "epoch [9/20] iter [1500/2944] loss [0.1603] elapsed time [107.64min]\n",
      "epoch [9/20] iter [1600/2944] loss [0.3974] elapsed time [108.00min]\n",
      "epoch [9/20] iter [1700/2944] loss [0.1299] elapsed time [108.36min]\n",
      "epoch [9/20] iter [1800/2944] loss [0.2376] elapsed time [108.72min]\n",
      "epoch [9/20] iter [1900/2944] loss [0.1490] elapsed time [109.08min]\n",
      "epoch [9/20] iter [2000/2944] loss [0.1888] elapsed time [109.44min]\n",
      "epoch [9/20] iter [2100/2944] loss [0.1232] elapsed time [109.80min]\n",
      "epoch [9/20] iter [2200/2944] loss [0.1040] elapsed time [110.16min]\n",
      "epoch [9/20] iter [2300/2944] loss [0.2922] elapsed time [110.52min]\n",
      "epoch [9/20] iter [2400/2944] loss [0.1666] elapsed time [110.88min]\n",
      "epoch [9/20] iter [2500/2944] loss [0.2681] elapsed time [111.24min]\n",
      "epoch [9/20] iter [2600/2944] loss [0.1535] elapsed time [111.60min]\n",
      "epoch [9/20] iter [2700/2944] loss [0.1314] elapsed time [111.95min]\n",
      "epoch [9/20] iter [2800/2944] loss [0.2288] elapsed time [112.31min]\n",
      "epoch [9/20] iter [2900/2944] loss [0.1493] elapsed time [112.67min]\n",
      "epoch [9/20] train loss [0.3274] validation loss [0.2158] elapsed time [112.83 min]\n",
      "\n",
      "epoch [10/20] iter [000/2944] loss [0.1227] elapsed time [112.84min]\n",
      "epoch [10/20] iter [100/2944] loss [0.1962] elapsed time [113.22min]\n",
      "epoch [10/20] iter [200/2944] loss [0.1342] elapsed time [113.60min]\n",
      "epoch [10/20] iter [300/2944] loss [0.1458] elapsed time [113.99min]\n",
      "epoch [10/20] iter [400/2944] loss [0.1681] elapsed time [114.36min]\n",
      "epoch [10/20] iter [500/2944] loss [0.1392] elapsed time [114.73min]\n",
      "epoch [10/20] iter [600/2944] loss [0.4022] elapsed time [115.10min]\n",
      "epoch [10/20] iter [700/2944] loss [0.6378] elapsed time [115.49min]\n",
      "epoch [10/20] iter [800/2944] loss [0.1549] elapsed time [115.89min]\n",
      "epoch [10/20] iter [900/2944] loss [0.3220] elapsed time [116.29min]\n",
      "epoch [10/20] iter [1000/2944] loss [0.2175] elapsed time [116.69min]\n",
      "epoch [10/20] iter [1100/2944] loss [0.1319] elapsed time [117.09min]\n",
      "epoch [10/20] iter [1200/2944] loss [0.1062] elapsed time [117.49min]\n",
      "epoch [10/20] iter [1300/2944] loss [0.1836] elapsed time [117.89min]\n",
      "epoch [10/20] iter [1400/2944] loss [0.5047] elapsed time [118.29min]\n",
      "epoch [10/20] iter [1500/2944] loss [0.1563] elapsed time [118.69min]\n",
      "epoch [10/20] iter [1600/2944] loss [1.0739] elapsed time [119.09min]\n",
      "epoch [10/20] iter [1700/2944] loss [1.0420] elapsed time [119.49min]\n",
      "epoch [10/20] iter [1800/2944] loss [0.2939] elapsed time [119.89min]\n",
      "epoch [10/20] iter [1900/2944] loss [0.1331] elapsed time [120.29min]\n",
      "epoch [10/20] iter [2000/2944] loss [0.1973] elapsed time [120.69min]\n",
      "epoch [10/20] iter [2100/2944] loss [0.2585] elapsed time [121.09min]\n",
      "epoch [10/20] iter [2200/2944] loss [0.1611] elapsed time [121.49min]\n",
      "epoch [10/20] iter [2300/2944] loss [0.8193] elapsed time [121.89min]\n",
      "epoch [10/20] iter [2400/2944] loss [0.8585] elapsed time [122.28min]\n",
      "epoch [10/20] iter [2500/2944] loss [0.1221] elapsed time [122.68min]\n",
      "epoch [10/20] iter [2600/2944] loss [0.1299] elapsed time [123.08min]\n",
      "epoch [10/20] iter [2700/2944] loss [0.1428] elapsed time [123.48min]\n",
      "epoch [10/20] iter [2800/2944] loss [0.1281] elapsed time [123.88min]\n",
      "epoch [10/20] iter [2900/2944] loss [0.2570] elapsed time [124.28min]\n",
      "epoch [10/20] train loss [0.3249] validation loss [0.2332] elapsed time [124.46 min]\n",
      "\n",
      "epoch [11/20] iter [000/2944] loss [0.1811] elapsed time [124.47min]\n",
      "epoch [11/20] iter [100/2944] loss [0.1714] elapsed time [124.86min]\n",
      "epoch [11/20] iter [200/2944] loss [0.2537] elapsed time [125.26min]\n",
      "epoch [11/20] iter [300/2944] loss [0.1483] elapsed time [125.66min]\n",
      "epoch [11/20] iter [400/2944] loss [0.1478] elapsed time [126.06min]\n",
      "epoch [11/20] iter [500/2944] loss [0.1783] elapsed time [126.46min]\n",
      "epoch [11/20] iter [600/2944] loss [0.2253] elapsed time [126.84min]\n",
      "epoch [11/20] iter [700/2944] loss [0.1480] elapsed time [127.23min]\n",
      "epoch [11/20] iter [800/2944] loss [0.1591] elapsed time [127.62min]\n",
      "epoch [11/20] iter [900/2944] loss [0.8925] elapsed time [128.01min]\n",
      "epoch [11/20] iter [1000/2944] loss [0.1960] elapsed time [128.39min]\n",
      "epoch [11/20] iter [1100/2944] loss [0.1324] elapsed time [128.77min]\n",
      "epoch [11/20] iter [1200/2944] loss [1.0189] elapsed time [129.16min]\n",
      "epoch [11/20] iter [1300/2944] loss [0.1434] elapsed time [129.55min]\n",
      "epoch [11/20] iter [1400/2944] loss [0.1928] elapsed time [129.95min]\n",
      "epoch [11/20] iter [1500/2944] loss [0.1608] elapsed time [130.35min]\n",
      "epoch [11/20] iter [1600/2944] loss [0.6140] elapsed time [130.73min]\n",
      "epoch [11/20] iter [1700/2944] loss [0.1296] elapsed time [131.11min]\n",
      "epoch [11/20] iter [1800/2944] loss [0.1241] elapsed time [131.50min]\n",
      "epoch [11/20] iter [1900/2944] loss [0.1407] elapsed time [131.88min]\n",
      "epoch [11/20] iter [2000/2944] loss [0.1796] elapsed time [132.27min]\n",
      "epoch [11/20] iter [2100/2944] loss [0.2108] elapsed time [132.67min]\n",
      "epoch [11/20] iter [2200/2944] loss [0.1338] elapsed time [133.06min]\n",
      "epoch [11/20] iter [2300/2944] loss [0.1603] elapsed time [133.47min]\n",
      "epoch [11/20] iter [2400/2944] loss [0.3220] elapsed time [133.85min]\n",
      "epoch [11/20] iter [2500/2944] loss [0.1647] elapsed time [134.24min]\n",
      "epoch [11/20] iter [2600/2944] loss [0.3713] elapsed time [134.62min]\n",
      "epoch [11/20] iter [2700/2944] loss [0.1523] elapsed time [135.00min]\n",
      "epoch [11/20] iter [2800/2944] loss [0.1458] elapsed time [135.38min]\n",
      "epoch [11/20] iter [2900/2944] loss [0.8962] elapsed time [135.77min]\n",
      "epoch [11/20] train loss [0.3240] validation loss [0.2521] elapsed time [135.95 min]\n",
      "\n",
      "epoch [12/20] iter [000/2944] loss [0.1485] elapsed time [135.96min]\n",
      "epoch [12/20] iter [100/2944] loss [0.2784] elapsed time [136.35min]\n",
      "epoch [12/20] iter [200/2944] loss [0.1633] elapsed time [136.73min]\n",
      "epoch [12/20] iter [300/2944] loss [0.1426] elapsed time [137.11min]\n",
      "epoch [12/20] iter [400/2944] loss [0.1266] elapsed time [137.49min]\n",
      "epoch [12/20] iter [500/2944] loss [0.1288] elapsed time [137.87min]\n",
      "epoch [12/20] iter [600/2944] loss [1.1132] elapsed time [138.26min]\n",
      "epoch [12/20] iter [700/2944] loss [0.1170] elapsed time [138.65min]\n",
      "epoch [12/20] iter [800/2944] loss [0.1434] elapsed time [139.03min]\n",
      "epoch [12/20] iter [900/2944] loss [0.1956] elapsed time [139.43min]\n",
      "epoch [12/20] iter [1000/2944] loss [0.4036] elapsed time [139.83min]\n",
      "epoch [12/20] iter [1100/2944] loss [2.1681] elapsed time [140.22min]\n",
      "epoch [12/20] iter [1200/2944] loss [0.3043] elapsed time [140.60min]\n",
      "epoch [12/20] iter [1300/2944] loss [0.1344] elapsed time [140.98min]\n",
      "epoch [12/20] iter [1400/2944] loss [0.1355] elapsed time [141.37min]\n",
      "epoch [12/20] iter [1500/2944] loss [1.9228] elapsed time [141.75min]\n",
      "epoch [12/20] iter [1600/2944] loss [0.1307] elapsed time [142.13min]\n",
      "epoch [12/20] iter [1700/2944] loss [0.1254] elapsed time [142.52min]\n",
      "epoch [12/20] iter [1800/2944] loss [0.1180] elapsed time [142.90min]\n",
      "epoch [12/20] iter [1900/2944] loss [0.1548] elapsed time [143.28min]\n",
      "epoch [12/20] iter [2000/2944] loss [0.3360] elapsed time [143.66min]\n",
      "epoch [12/20] iter [2100/2944] loss [1.4433] elapsed time [144.04min]\n",
      "epoch [12/20] iter [2200/2944] loss [1.6244] elapsed time [144.42min]\n",
      "epoch [12/20] iter [2300/2944] loss [0.7269] elapsed time [144.80min]\n",
      "epoch [12/20] iter [2400/2944] loss [0.7843] elapsed time [145.18min]\n",
      "epoch [12/20] iter [2500/2944] loss [0.1349] elapsed time [145.56min]\n",
      "epoch [12/20] iter [2600/2944] loss [0.1452] elapsed time [145.95min]\n",
      "epoch [12/20] iter [2700/2944] loss [0.1247] elapsed time [146.33min]\n",
      "epoch [12/20] iter [2800/2944] loss [0.1597] elapsed time [146.72min]\n",
      "epoch [12/20] iter [2900/2944] loss [1.0070] elapsed time [147.11min]\n",
      "epoch [12/20] train loss [0.3212] validation loss [0.2225] elapsed time [147.28 min]\n",
      "\n",
      "epoch [13/20] iter [000/2944] loss [0.1088] elapsed time [147.29min]\n",
      "epoch [13/20] iter [100/2944] loss [0.2180] elapsed time [147.68min]\n",
      "epoch [13/20] iter [200/2944] loss [0.1259] elapsed time [148.06min]\n",
      "epoch [13/20] iter [300/2944] loss [0.1875] elapsed time [148.45min]\n",
      "epoch [13/20] iter [400/2944] loss [1.1023] elapsed time [148.83min]\n",
      "epoch [13/20] iter [500/2944] loss [0.1265] elapsed time [149.21min]\n",
      "epoch [13/20] iter [600/2944] loss [0.1276] elapsed time [149.60min]\n",
      "epoch [13/20] iter [700/2944] loss [0.1384] elapsed time [149.98min]\n",
      "epoch [13/20] iter [800/2944] loss [0.2363] elapsed time [150.37min]\n",
      "epoch [13/20] iter [900/2944] loss [0.1306] elapsed time [150.78min]\n",
      "epoch [13/20] iter [1000/2944] loss [0.1293] elapsed time [151.18min]\n",
      "epoch [13/20] iter [1100/2944] loss [0.1135] elapsed time [151.59min]\n",
      "epoch [13/20] iter [1200/2944] loss [0.1383] elapsed time [152.00min]\n",
      "epoch [13/20] iter [1300/2944] loss [0.3977] elapsed time [152.41min]\n",
      "epoch [13/20] iter [1400/2944] loss [0.1607] elapsed time [152.82min]\n",
      "epoch [13/20] iter [1500/2944] loss [0.1511] elapsed time [153.23min]\n",
      "epoch [13/20] iter [1600/2944] loss [0.1625] elapsed time [153.63min]\n",
      "epoch [13/20] iter [1700/2944] loss [0.1239] elapsed time [154.04min]\n",
      "epoch [13/20] iter [1800/2944] loss [0.1230] elapsed time [154.45min]\n",
      "epoch [13/20] iter [1900/2944] loss [0.2056] elapsed time [154.85min]\n",
      "epoch [13/20] iter [2000/2944] loss [0.1280] elapsed time [155.27min]\n",
      "epoch [13/20] iter [2100/2944] loss [0.1445] elapsed time [155.68min]\n",
      "epoch [13/20] iter [2200/2944] loss [0.1490] elapsed time [156.09min]\n",
      "epoch [13/20] iter [2300/2944] loss [0.1578] elapsed time [156.51min]\n",
      "epoch [13/20] iter [2400/2944] loss [0.1548] elapsed time [156.92min]\n",
      "epoch [13/20] iter [2500/2944] loss [0.1494] elapsed time [157.33min]\n",
      "epoch [13/20] iter [2600/2944] loss [0.1194] elapsed time [157.74min]\n",
      "epoch [13/20] iter [2700/2944] loss [0.1698] elapsed time [158.15min]\n",
      "epoch [13/20] iter [2800/2944] loss [0.1295] elapsed time [158.57min]\n",
      "epoch [13/20] iter [2900/2944] loss [0.1175] elapsed time [158.98min]\n",
      "epoch [13/20] train loss [0.3207] validation loss [0.2199] elapsed time [159.16 min]\n",
      "\n",
      "epoch [14/20] iter [000/2944] loss [0.2471] elapsed time [159.17min]\n",
      "epoch [14/20] iter [100/2944] loss [0.1860] elapsed time [159.58min]\n",
      "epoch [14/20] iter [200/2944] loss [2.1076] elapsed time [159.99min]\n",
      "epoch [14/20] iter [300/2944] loss [0.1170] elapsed time [160.40min]\n",
      "epoch [14/20] iter [400/2944] loss [0.2615] elapsed time [160.81min]\n",
      "epoch [14/20] iter [500/2944] loss [0.1453] elapsed time [161.22min]\n",
      "epoch [14/20] iter [600/2944] loss [0.1749] elapsed time [161.63min]\n",
      "epoch [14/20] iter [700/2944] loss [0.2875] elapsed time [162.04min]\n",
      "epoch [14/20] iter [800/2944] loss [0.1490] elapsed time [162.46min]\n",
      "epoch [14/20] iter [900/2944] loss [0.1364] elapsed time [162.84min]\n",
      "epoch [14/20] iter [1000/2944] loss [1.0945] elapsed time [163.21min]\n",
      "epoch [14/20] iter [1100/2944] loss [0.1160] elapsed time [163.57min]\n",
      "epoch [14/20] iter [1200/2944] loss [0.1927] elapsed time [163.93min]\n",
      "epoch [14/20] iter [1300/2944] loss [0.1327] elapsed time [164.28min]\n",
      "epoch [14/20] iter [1400/2944] loss [0.1799] elapsed time [164.64min]\n",
      "epoch [14/20] iter [1500/2944] loss [0.1254] elapsed time [165.00min]\n",
      "epoch [14/20] iter [1600/2944] loss [0.1788] elapsed time [165.36min]\n",
      "epoch [14/20] iter [1700/2944] loss [0.1451] elapsed time [165.72min]\n",
      "epoch [14/20] iter [1800/2944] loss [0.1730] elapsed time [166.07min]\n",
      "epoch [14/20] iter [1900/2944] loss [0.1268] elapsed time [166.43min]\n",
      "epoch [14/20] iter [2000/2944] loss [0.1741] elapsed time [166.79min]\n",
      "epoch [14/20] iter [2100/2944] loss [0.1808] elapsed time [167.15min]\n",
      "epoch [14/20] iter [2200/2944] loss [0.1554] elapsed time [167.51min]\n",
      "epoch [14/20] iter [2300/2944] loss [0.1854] elapsed time [167.87min]\n",
      "epoch [14/20] iter [2400/2944] loss [0.2249] elapsed time [168.23min]\n",
      "epoch [14/20] iter [2500/2944] loss [0.1544] elapsed time [168.58min]\n",
      "epoch [14/20] iter [2600/2944] loss [0.1555] elapsed time [168.94min]\n",
      "epoch [14/20] iter [2700/2944] loss [1.0936] elapsed time [169.29min]\n",
      "epoch [14/20] iter [2800/2944] loss [0.1170] elapsed time [169.65min]\n",
      "epoch [14/20] iter [2900/2944] loss [1.6824] elapsed time [170.01min]\n",
      "epoch [14/20] train loss [0.3204] validation loss [0.2169] elapsed time [170.17 min]\n",
      "\n",
      "epoch [15/20] iter [000/2944] loss [0.1920] elapsed time [170.18min]\n",
      "epoch [15/20] iter [100/2944] loss [0.1413] elapsed time [170.56min]\n",
      "epoch [15/20] iter [200/2944] loss [0.1429] elapsed time [170.95min]\n",
      "epoch [15/20] iter [300/2944] loss [0.1463] elapsed time [171.34min]\n",
      "epoch [15/20] iter [400/2944] loss [0.1231] elapsed time [171.72min]\n",
      "epoch [15/20] iter [500/2944] loss [0.1220] elapsed time [172.09min]\n",
      "epoch [15/20] iter [600/2944] loss [0.1849] elapsed time [172.46min]\n",
      "epoch [15/20] iter [700/2944] loss [0.1535] elapsed time [172.82min]\n",
      "epoch [15/20] iter [800/2944] loss [0.1593] elapsed time [173.18min]\n",
      "epoch [15/20] iter [900/2944] loss [0.4506] elapsed time [173.55min]\n",
      "epoch [15/20] iter [1000/2944] loss [0.1543] elapsed time [173.92min]\n",
      "epoch [15/20] iter [1100/2944] loss [0.1766] elapsed time [174.29min]\n",
      "epoch [15/20] iter [1200/2944] loss [0.1410] elapsed time [174.66min]\n",
      "epoch [15/20] iter [1300/2944] loss [0.1154] elapsed time [175.02min]\n",
      "epoch [15/20] iter [1400/2944] loss [0.7356] elapsed time [175.40min]\n",
      "epoch [15/20] iter [1500/2944] loss [0.1264] elapsed time [175.76min]\n",
      "epoch [15/20] iter [1600/2944] loss [0.1343] elapsed time [176.11min]\n",
      "epoch [15/20] iter [1700/2944] loss [0.1347] elapsed time [176.47min]\n",
      "epoch [15/20] iter [1800/2944] loss [0.1194] elapsed time [176.84min]\n",
      "epoch [15/20] iter [1900/2944] loss [0.1391] elapsed time [177.19min]\n",
      "epoch [15/20] iter [2000/2944] loss [0.1116] elapsed time [177.56min]\n",
      "epoch [15/20] iter [2100/2944] loss [0.1193] elapsed time [177.92min]\n",
      "epoch [15/20] iter [2200/2944] loss [0.2417] elapsed time [178.28min]\n",
      "epoch [15/20] iter [2300/2944] loss [0.3847] elapsed time [178.64min]\n",
      "epoch [15/20] iter [2400/2944] loss [1.0382] elapsed time [179.00min]\n",
      "epoch [15/20] iter [2500/2944] loss [0.2069] elapsed time [179.35min]\n",
      "epoch [15/20] iter [2600/2944] loss [0.1232] elapsed time [179.71min]\n",
      "epoch [15/20] iter [2700/2944] loss [0.2540] elapsed time [180.07min]\n",
      "epoch [15/20] iter [2800/2944] loss [0.0978] elapsed time [180.43min]\n",
      "epoch [15/20] iter [2900/2944] loss [0.1540] elapsed time [180.79min]\n",
      "epoch [15/20] train loss [0.3181] validation loss [0.2167] elapsed time [180.95 min]\n",
      "\n",
      "epoch [16/20] iter [000/2944] loss [0.2030] elapsed time [180.96min]\n",
      "epoch [16/20] iter [100/2944] loss [0.1148] elapsed time [181.35min]\n",
      "epoch [16/20] iter [200/2944] loss [0.1547] elapsed time [181.74min]\n",
      "epoch [16/20] iter [300/2944] loss [0.1402] elapsed time [182.11min]\n",
      "epoch [16/20] iter [400/2944] loss [0.2641] elapsed time [182.49min]\n",
      "epoch [16/20] iter [500/2944] loss [0.1781] elapsed time [182.87min]\n",
      "epoch [16/20] iter [600/2944] loss [0.1154] elapsed time [183.26min]\n",
      "epoch [16/20] iter [700/2944] loss [0.1936] elapsed time [183.66min]\n",
      "epoch [16/20] iter [800/2944] loss [0.1485] elapsed time [184.05min]\n",
      "epoch [16/20] iter [900/2944] loss [0.1296] elapsed time [184.45min]\n",
      "epoch [16/20] iter [1000/2944] loss [0.1369] elapsed time [184.86min]\n",
      "epoch [16/20] iter [1100/2944] loss [0.1154] elapsed time [185.26min]\n",
      "epoch [16/20] iter [1200/2944] loss [0.2097] elapsed time [185.67min]\n",
      "epoch [16/20] iter [1300/2944] loss [0.1131] elapsed time [186.07min]\n",
      "epoch [16/20] iter [1400/2944] loss [0.1211] elapsed time [186.47min]\n",
      "epoch [16/20] iter [1500/2944] loss [0.2336] elapsed time [186.88min]\n",
      "epoch [16/20] iter [1600/2944] loss [0.1904] elapsed time [187.28min]\n",
      "epoch [16/20] iter [1700/2944] loss [0.1175] elapsed time [187.69min]\n",
      "epoch [16/20] iter [1800/2944] loss [0.8133] elapsed time [188.09min]\n",
      "epoch [16/20] iter [1900/2944] loss [0.1262] elapsed time [188.49min]\n",
      "epoch [16/20] iter [2000/2944] loss [0.1852] elapsed time [188.88min]\n",
      "epoch [16/20] iter [2100/2944] loss [0.1463] elapsed time [189.28min]\n",
      "epoch [16/20] iter [2200/2944] loss [0.1400] elapsed time [189.67min]\n",
      "epoch [16/20] iter [2300/2944] loss [0.1617] elapsed time [190.07min]\n",
      "epoch [16/20] iter [2400/2944] loss [0.1433] elapsed time [190.47min]\n",
      "epoch [16/20] iter [2500/2944] loss [1.5035] elapsed time [190.87min]\n",
      "epoch [16/20] iter [2600/2944] loss [0.1264] elapsed time [191.28min]\n",
      "epoch [16/20] iter [2700/2944] loss [0.1347] elapsed time [191.68min]\n",
      "epoch [16/20] iter [2800/2944] loss [0.3555] elapsed time [192.09min]\n",
      "epoch [16/20] iter [2900/2944] loss [0.8047] elapsed time [192.50min]\n",
      "epoch [16/20] train loss [0.3189] validation loss [0.2032] elapsed time [192.68 min]\n",
      "\n",
      "epoch [17/20] iter [000/2944] loss [0.1421] elapsed time [192.69min]\n",
      "epoch [17/20] iter [100/2944] loss [0.2738] elapsed time [193.07min]\n",
      "epoch [17/20] iter [200/2944] loss [0.1522] elapsed time [193.45min]\n",
      "epoch [17/20] iter [300/2944] loss [0.1045] elapsed time [193.84min]\n",
      "epoch [17/20] iter [400/2944] loss [0.1986] elapsed time [194.22min]\n",
      "epoch [17/20] iter [500/2944] loss [0.1138] elapsed time [194.62min]\n",
      "epoch [17/20] iter [600/2944] loss [0.4361] elapsed time [195.01min]\n",
      "epoch [17/20] iter [700/2944] loss [0.1654] elapsed time [195.41min]\n",
      "epoch [17/20] iter [800/2944] loss [0.1420] elapsed time [195.79min]\n",
      "epoch [17/20] iter [900/2944] loss [0.8217] elapsed time [196.17min]\n",
      "epoch [17/20] iter [1000/2944] loss [0.1182] elapsed time [196.57min]\n",
      "epoch [17/20] iter [1100/2944] loss [0.3131] elapsed time [196.96min]\n",
      "epoch [17/20] iter [1200/2944] loss [0.1632] elapsed time [197.41min]\n",
      "epoch [17/20] iter [1300/2944] loss [0.4865] elapsed time [197.81min]\n",
      "epoch [17/20] iter [1400/2944] loss [0.1358] elapsed time [198.20min]\n",
      "epoch [17/20] iter [1500/2944] loss [0.1963] elapsed time [198.60min]\n",
      "epoch [17/20] iter [1600/2944] loss [1.9067] elapsed time [199.00min]\n",
      "epoch [17/20] iter [1700/2944] loss [0.1255] elapsed time [199.40min]\n",
      "epoch [17/20] iter [1800/2944] loss [0.3601] elapsed time [199.80min]\n",
      "epoch [17/20] iter [1900/2944] loss [0.1356] elapsed time [200.18min]\n",
      "epoch [17/20] iter [2000/2944] loss [0.1441] elapsed time [200.57min]\n",
      "epoch [17/20] iter [2100/2944] loss [0.1971] elapsed time [200.96min]\n",
      "epoch [17/20] iter [2200/2944] loss [0.1396] elapsed time [201.36min]\n",
      "epoch [17/20] iter [2300/2944] loss [0.1431] elapsed time [201.77min]\n",
      "epoch [17/20] iter [2400/2944] loss [0.1423] elapsed time [202.16min]\n",
      "epoch [17/20] iter [2500/2944] loss [0.1148] elapsed time [202.55min]\n",
      "epoch [17/20] iter [2600/2944] loss [0.1326] elapsed time [202.94min]\n",
      "epoch [17/20] iter [2700/2944] loss [0.1381] elapsed time [203.32min]\n",
      "epoch [17/20] iter [2800/2944] loss [0.1319] elapsed time [203.73min]\n",
      "epoch [17/20] iter [2900/2944] loss [0.1746] elapsed time [204.12min]\n",
      "epoch [17/20] train loss [0.3200] validation loss [0.2574] elapsed time [204.31 min]\n",
      "\n",
      "epoch [18/20] iter [000/2944] loss [1.8628] elapsed time [204.31min]\n",
      "epoch [18/20] iter [100/2944] loss [0.1245] elapsed time [204.70min]\n",
      "epoch [18/20] iter [200/2944] loss [0.1953] elapsed time [205.08min]\n",
      "epoch [18/20] iter [300/2944] loss [0.2287] elapsed time [205.46min]\n",
      "epoch [18/20] iter [400/2944] loss [0.4016] elapsed time [205.85min]\n",
      "epoch [18/20] iter [500/2944] loss [0.1426] elapsed time [206.22min]\n",
      "epoch [18/20] iter [600/2944] loss [0.1383] elapsed time [206.59min]\n",
      "epoch [18/20] iter [700/2944] loss [1.1283] elapsed time [206.96min]\n",
      "epoch [18/20] iter [800/2944] loss [2.5009] elapsed time [207.35min]\n",
      "epoch [18/20] iter [900/2944] loss [1.0151] elapsed time [207.74min]\n",
      "epoch [18/20] iter [1000/2944] loss [0.9730] elapsed time [208.13min]\n",
      "epoch [18/20] iter [1100/2944] loss [0.9243] elapsed time [208.52min]\n",
      "epoch [18/20] iter [1200/2944] loss [1.0100] elapsed time [208.91min]\n",
      "epoch [18/20] iter [1300/2944] loss [0.9118] elapsed time [209.28min]\n",
      "epoch [18/20] iter [1400/2944] loss [1.0254] elapsed time [209.65min]\n",
      "epoch [18/20] iter [1500/2944] loss [2.1016] elapsed time [210.03min]\n",
      "epoch [18/20] iter [1600/2944] loss [0.6894] elapsed time [210.42min]\n",
      "epoch [18/20] iter [1700/2944] loss [0.9209] elapsed time [210.79min]\n",
      "epoch [18/20] iter [1800/2944] loss [1.0502] elapsed time [211.15min]\n",
      "epoch [18/20] iter [1900/2944] loss [0.9542] elapsed time [211.52min]\n",
      "epoch [18/20] iter [2000/2944] loss [0.9294] elapsed time [211.89min]\n",
      "epoch [18/20] iter [2100/2944] loss [0.8835] elapsed time [212.26min]\n",
      "epoch [18/20] iter [2200/2944] loss [2.1927] elapsed time [212.64min]\n",
      "epoch [18/20] iter [2300/2944] loss [0.8594] elapsed time [213.01min]\n",
      "epoch [18/20] iter [2400/2944] loss [0.8659] elapsed time [213.37min]\n",
      "epoch [18/20] iter [2500/2944] loss [0.6612] elapsed time [213.73min]\n",
      "epoch [18/20] iter [2600/2944] loss [0.8607] elapsed time [214.09min]\n",
      "epoch [18/20] iter [2700/2944] loss [0.8667] elapsed time [214.45min]\n",
      "epoch [18/20] iter [2800/2944] loss [0.7662] elapsed time [214.81min]\n",
      "epoch [18/20] iter [2900/2944] loss [0.6846] elapsed time [215.17min]\n",
      "epoch [18/20] train loss [0.9137] validation loss [1.0326] elapsed time [215.33 min]\n",
      "\n",
      "epoch [19/20] iter [000/2944] loss [2.1739] elapsed time [215.34min]\n",
      "epoch [19/20] iter [100/2944] loss [0.9205] elapsed time [215.71min]\n",
      "epoch [19/20] iter [200/2944] loss [0.8061] elapsed time [216.08min]\n",
      "epoch [19/20] iter [300/2944] loss [1.6132] elapsed time [216.46min]\n",
      "epoch [19/20] iter [400/2944] loss [0.8521] elapsed time [216.83min]\n",
      "epoch [19/20] iter [500/2944] loss [0.8987] elapsed time [217.19min]\n",
      "epoch [19/20] iter [600/2944] loss [0.7441] elapsed time [217.55min]\n",
      "epoch [19/20] iter [700/2944] loss [0.8595] elapsed time [217.92min]\n",
      "epoch [19/20] iter [800/2944] loss [1.2106] elapsed time [218.28min]\n",
      "epoch [19/20] iter [900/2944] loss [2.1307] elapsed time [218.66min]\n",
      "epoch [19/20] iter [1000/2944] loss [0.9739] elapsed time [219.04min]\n",
      "epoch [19/20] iter [1100/2944] loss [0.4512] elapsed time [219.40min]\n",
      "epoch [19/20] iter [1200/2944] loss [0.4187] elapsed time [219.76min]\n",
      "epoch [19/20] iter [1300/2944] loss [0.4462] elapsed time [220.12min]\n",
      "epoch [19/20] iter [1400/2944] loss [1.2243] elapsed time [220.48min]\n",
      "epoch [19/20] iter [1500/2944] loss [0.8929] elapsed time [220.84min]\n",
      "epoch [19/20] iter [1600/2944] loss [0.3580] elapsed time [221.21min]\n",
      "epoch [19/20] iter [1700/2944] loss [0.3404] elapsed time [221.57min]\n",
      "epoch [19/20] iter [1800/2944] loss [0.2860] elapsed time [221.93min]\n",
      "epoch [19/20] iter [1900/2944] loss [0.3828] elapsed time [222.30min]\n",
      "epoch [19/20] iter [2000/2944] loss [0.2503] elapsed time [222.65min]\n",
      "epoch [19/20] iter [2100/2944] loss [0.2507] elapsed time [223.01min]\n",
      "epoch [19/20] iter [2200/2944] loss [0.1826] elapsed time [223.37min]\n",
      "epoch [19/20] iter [2300/2944] loss [0.3202] elapsed time [223.73min]\n",
      "epoch [19/20] iter [2400/2944] loss [0.2144] elapsed time [224.09min]\n",
      "epoch [19/20] iter [2500/2944] loss [0.2691] elapsed time [224.45min]\n",
      "epoch [19/20] iter [2600/2944] loss [0.3389] elapsed time [224.81min]\n",
      "epoch [19/20] iter [2700/2944] loss [0.2656] elapsed time [225.16min]\n",
      "epoch [19/20] iter [2800/2944] loss [0.1782] elapsed time [225.52min]\n",
      "epoch [19/20] iter [2900/2944] loss [0.4031] elapsed time [225.88min]\n",
      "epoch [19/20] train loss [0.6347] validation loss [0.3112] elapsed time [226.04 min]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_epoch = 0\n",
    "best_model_state_dict = None\n",
    "best_val_loss = 10000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss_mean = 0\n",
    "    val_loss_mean = 0\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        forecast = model(inputs)\n",
    "        loss = criterion(forecast.sum(0), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_mean += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('epoch [{}/{}] iter [{:03d}/{:03d}] loss [{:.4f}] elapsed time [{:.2f}min]'.format(epoch, EPOCHS, i, len(train_dataloader), loss.item(), (time.time()-start_time)/60))\n",
    "    \n",
    "    train_loss_mean = train_loss_mean/len(train_dataloader)\n",
    "    \n",
    "    train_losses.append(train_loss_mean)\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(val_dataloader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            forecast = model(inputs)\n",
    "            loss = criterion(forecast.sum(0), labels)\n",
    "            val_loss_mean += loss.item()\n",
    "\n",
    "        val_loss_mean = val_loss_mean/len(val_dataloader)\n",
    "\n",
    "        print('epoch [{}/{}] train loss [{:.4f}] validation loss [{:.4f}] elapsed time [{:.2f} min]\\n'.format(epoch, EPOCHS, train_loss_mean, val_loss_mean, (time.time()-start_time)/60))\n",
    "\n",
    "        if val_loss_mean < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = val_loss_mean\n",
    "            torch.save(model.state_dict(), \"best-model.pt\")\n",
    "    val_losses.append(val_loss_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b921d4-3f77-4807-8650-c3c42dae40eb",
   "metadata": {},
   "source": [
    "#### Validation loss 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dca70984-9ceb-4009-b13e-f8d3faa3173e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.axvline(best_epoch, color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d920998-a551-4d47-8685-b936a44d8c3b",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1437990-6227-40e9-aa21-22902c89a5df",
   "metadata": {},
   "source": [
    "#### 테스트 Dataset, DataLoader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ef90b3-4eb9-4899-970a-f8f69c3d25be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TimeseriesDataset(test_df, BACKCAST_LENGTH, FORECAST_LENGTH, scaler=train_dataset.scaler)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                               num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7680740-8500-4beb-a62f-d534d16109a0",
   "metadata": {},
   "source": [
    "#### 최고 성능 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e725318b-2713-4af6-a288-58e21d7adb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d6f29-ad65-4582-88fa-5a938f45fb9c",
   "metadata": {},
   "source": [
    "#### 추론 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf3ce660-df96-4b02-8704-153f9e4b10e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_dataloader):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        forecast = model(inputs)\n",
    "        predictions.append(forecast.data.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d93e54d5-88d8-426a-8710-f76f4b535a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([168, 35])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.scaler.unscale(predictions)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10e91db1-3fb7-4b92-b5b0-9e7f08cbca6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([168, 35])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.scaler.unscale(predictions)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed17741-0d25-48e9-898d-a6f792352bef",
   "metadata": {},
   "source": [
    "#### 예측 결과 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9ca41d6-3c83-4ec9-a5f3-bbe18b5531b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>200</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200525_0</td>\n",
       "      <td>67566.807681</td>\n",
       "      <td>11460.134929</td>\n",
       "      <td>983.650270</td>\n",
       "      <td>2769.467384</td>\n",
       "      <td>912.187063</td>\n",
       "      <td>703.466816</td>\n",
       "      <td>25051.738730</td>\n",
       "      <td>767.324189</td>\n",
       "      <td>1891.354923</td>\n",
       "      <td>...</td>\n",
       "      <td>478.763843</td>\n",
       "      <td>2383.593180</td>\n",
       "      <td>5410.813118</td>\n",
       "      <td>4098.738700</td>\n",
       "      <td>1191.321644</td>\n",
       "      <td>3370.409520</td>\n",
       "      <td>267.786028</td>\n",
       "      <td>1479.324829</td>\n",
       "      <td>1336.970471</td>\n",
       "      <td>1889.604904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200525_1</td>\n",
       "      <td>55048.754359</td>\n",
       "      <td>9252.971021</td>\n",
       "      <td>762.899229</td>\n",
       "      <td>2086.298798</td>\n",
       "      <td>710.089688</td>\n",
       "      <td>556.844239</td>\n",
       "      <td>20464.153371</td>\n",
       "      <td>631.502118</td>\n",
       "      <td>1584.469499</td>\n",
       "      <td>...</td>\n",
       "      <td>353.161724</td>\n",
       "      <td>2074.249319</td>\n",
       "      <td>4063.783008</td>\n",
       "      <td>3413.819670</td>\n",
       "      <td>969.187485</td>\n",
       "      <td>2900.235893</td>\n",
       "      <td>237.712564</td>\n",
       "      <td>1239.254616</td>\n",
       "      <td>1097.984215</td>\n",
       "      <td>1575.578501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200525_2</td>\n",
       "      <td>49353.142323</td>\n",
       "      <td>8431.548222</td>\n",
       "      <td>662.385937</td>\n",
       "      <td>1673.380586</td>\n",
       "      <td>583.498046</td>\n",
       "      <td>478.592386</td>\n",
       "      <td>18037.560720</td>\n",
       "      <td>576.819139</td>\n",
       "      <td>1487.624684</td>\n",
       "      <td>...</td>\n",
       "      <td>281.288069</td>\n",
       "      <td>1937.219272</td>\n",
       "      <td>3396.567303</td>\n",
       "      <td>3083.361653</td>\n",
       "      <td>822.255289</td>\n",
       "      <td>2623.984029</td>\n",
       "      <td>238.233660</td>\n",
       "      <td>1212.557481</td>\n",
       "      <td>990.063390</td>\n",
       "      <td>1429.250499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200525_3</td>\n",
       "      <td>49037.175865</td>\n",
       "      <td>8657.223919</td>\n",
       "      <td>646.201069</td>\n",
       "      <td>1432.574901</td>\n",
       "      <td>480.654454</td>\n",
       "      <td>433.920537</td>\n",
       "      <td>16420.419829</td>\n",
       "      <td>573.541442</td>\n",
       "      <td>1453.838560</td>\n",
       "      <td>...</td>\n",
       "      <td>205.287210</td>\n",
       "      <td>1861.499879</td>\n",
       "      <td>3121.522660</td>\n",
       "      <td>2878.243453</td>\n",
       "      <td>716.963879</td>\n",
       "      <td>2309.397095</td>\n",
       "      <td>246.475241</td>\n",
       "      <td>1279.705455</td>\n",
       "      <td>953.161538</td>\n",
       "      <td>1415.914875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200525_4</td>\n",
       "      <td>60068.606203</td>\n",
       "      <td>11511.549870</td>\n",
       "      <td>428.654854</td>\n",
       "      <td>1955.002517</td>\n",
       "      <td>440.722326</td>\n",
       "      <td>629.164214</td>\n",
       "      <td>19976.879923</td>\n",
       "      <td>552.172245</td>\n",
       "      <td>1372.865066</td>\n",
       "      <td>...</td>\n",
       "      <td>357.633477</td>\n",
       "      <td>2264.402034</td>\n",
       "      <td>5311.748371</td>\n",
       "      <td>3042.146560</td>\n",
       "      <td>823.289726</td>\n",
       "      <td>2318.923533</td>\n",
       "      <td>280.901976</td>\n",
       "      <td>1578.093550</td>\n",
       "      <td>1181.196390</td>\n",
       "      <td>2459.799594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>20200531_19</td>\n",
       "      <td>277020.556648</td>\n",
       "      <td>71984.930661</td>\n",
       "      <td>7176.315921</td>\n",
       "      <td>17958.535225</td>\n",
       "      <td>7365.684664</td>\n",
       "      <td>5206.093376</td>\n",
       "      <td>113060.422974</td>\n",
       "      <td>4946.118562</td>\n",
       "      <td>12179.675761</td>\n",
       "      <td>...</td>\n",
       "      <td>5632.111612</td>\n",
       "      <td>13052.873768</td>\n",
       "      <td>33256.701991</td>\n",
       "      <td>18058.628689</td>\n",
       "      <td>5482.338019</td>\n",
       "      <td>16973.545140</td>\n",
       "      <td>2855.807821</td>\n",
       "      <td>13466.158350</td>\n",
       "      <td>8605.197958</td>\n",
       "      <td>15150.040284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>20200531_20</td>\n",
       "      <td>244865.671109</td>\n",
       "      <td>55236.419541</td>\n",
       "      <td>5110.074396</td>\n",
       "      <td>12927.587142</td>\n",
       "      <td>5042.084436</td>\n",
       "      <td>3691.539605</td>\n",
       "      <td>96790.936363</td>\n",
       "      <td>3687.446803</td>\n",
       "      <td>9035.983942</td>\n",
       "      <td>...</td>\n",
       "      <td>3881.099056</td>\n",
       "      <td>10506.889078</td>\n",
       "      <td>28213.065690</td>\n",
       "      <td>15709.712814</td>\n",
       "      <td>4451.614922</td>\n",
       "      <td>13205.617957</td>\n",
       "      <td>1917.205749</td>\n",
       "      <td>9414.607451</td>\n",
       "      <td>6428.157459</td>\n",
       "      <td>10995.838902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>20200531_21</td>\n",
       "      <td>208339.705391</td>\n",
       "      <td>42387.122527</td>\n",
       "      <td>3638.479275</td>\n",
       "      <td>9018.944351</td>\n",
       "      <td>3353.981591</td>\n",
       "      <td>2571.775021</td>\n",
       "      <td>79698.347180</td>\n",
       "      <td>2712.214079</td>\n",
       "      <td>6578.501232</td>\n",
       "      <td>...</td>\n",
       "      <td>2650.130905</td>\n",
       "      <td>7756.237717</td>\n",
       "      <td>22599.452866</td>\n",
       "      <td>12739.017214</td>\n",
       "      <td>3438.028237</td>\n",
       "      <td>10220.189805</td>\n",
       "      <td>1314.046429</td>\n",
       "      <td>6650.817474</td>\n",
       "      <td>4821.256399</td>\n",
       "      <td>7842.087456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>20200531_22</td>\n",
       "      <td>161379.087000</td>\n",
       "      <td>30377.541644</td>\n",
       "      <td>2567.075869</td>\n",
       "      <td>6456.797282</td>\n",
       "      <td>2181.482586</td>\n",
       "      <td>1759.418759</td>\n",
       "      <td>59538.483772</td>\n",
       "      <td>1957.248979</td>\n",
       "      <td>4631.926479</td>\n",
       "      <td>...</td>\n",
       "      <td>1708.101388</td>\n",
       "      <td>5273.201435</td>\n",
       "      <td>15775.263871</td>\n",
       "      <td>9468.200727</td>\n",
       "      <td>2457.632547</td>\n",
       "      <td>7479.922532</td>\n",
       "      <td>861.730292</td>\n",
       "      <td>4503.500760</td>\n",
       "      <td>3402.110653</td>\n",
       "      <td>5208.806618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>20200531_23</td>\n",
       "      <td>110832.604121</td>\n",
       "      <td>19561.698816</td>\n",
       "      <td>1696.186941</td>\n",
       "      <td>4598.768576</td>\n",
       "      <td>1456.842262</td>\n",
       "      <td>1171.014256</td>\n",
       "      <td>40417.033028</td>\n",
       "      <td>1298.944633</td>\n",
       "      <td>3073.080585</td>\n",
       "      <td>...</td>\n",
       "      <td>979.644477</td>\n",
       "      <td>3568.745768</td>\n",
       "      <td>9933.802338</td>\n",
       "      <td>6479.700047</td>\n",
       "      <td>1741.749364</td>\n",
       "      <td>5065.378998</td>\n",
       "      <td>499.849024</td>\n",
       "      <td>2728.745200</td>\n",
       "      <td>2216.997636</td>\n",
       "      <td>3224.551945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp             10           100          101           120  \\\n",
       "0     20200525_0   67566.807681  11460.134929   983.650270   2769.467384   \n",
       "1     20200525_1   55048.754359   9252.971021   762.899229   2086.298798   \n",
       "2     20200525_2   49353.142323   8431.548222   662.385937   1673.380586   \n",
       "3     20200525_3   49037.175865   8657.223919   646.201069   1432.574901   \n",
       "4     20200525_4   60068.606203  11511.549870   428.654854   1955.002517   \n",
       "..           ...            ...           ...          ...           ...   \n",
       "163  20200531_19  277020.556648  71984.930661  7176.315921  17958.535225   \n",
       "164  20200531_20  244865.671109  55236.419541  5110.074396  12927.587142   \n",
       "165  20200531_21  208339.705391  42387.122527  3638.479275   9018.944351   \n",
       "166  20200531_22  161379.087000  30377.541644  2567.075869   6456.797282   \n",
       "167  20200531_23  110832.604121  19561.698816  1696.186941   4598.768576   \n",
       "\n",
       "             121          140            150          160           200  ...  \\\n",
       "0     912.187063   703.466816   25051.738730   767.324189   1891.354923  ...   \n",
       "1     710.089688   556.844239   20464.153371   631.502118   1584.469499  ...   \n",
       "2     583.498046   478.592386   18037.560720   576.819139   1487.624684  ...   \n",
       "3     480.654454   433.920537   16420.419829   573.541442   1453.838560  ...   \n",
       "4     440.722326   629.164214   19976.879923   552.172245   1372.865066  ...   \n",
       "..           ...          ...            ...          ...           ...  ...   \n",
       "163  7365.684664  5206.093376  113060.422974  4946.118562  12179.675761  ...   \n",
       "164  5042.084436  3691.539605   96790.936363  3687.446803   9035.983942  ...   \n",
       "165  3353.981591  2571.775021   79698.347180  2712.214079   6578.501232  ...   \n",
       "166  2181.482586  1759.418759   59538.483772  1957.248979   4631.926479  ...   \n",
       "167  1456.842262  1171.014256   40417.033028  1298.944633   3073.080585  ...   \n",
       "\n",
       "            1020          1040          1100          1200         1510  \\\n",
       "0     478.763843   2383.593180   5410.813118   4098.738700  1191.321644   \n",
       "1     353.161724   2074.249319   4063.783008   3413.819670   969.187485   \n",
       "2     281.288069   1937.219272   3396.567303   3083.361653   822.255289   \n",
       "3     205.287210   1861.499879   3121.522660   2878.243453   716.963879   \n",
       "4     357.633477   2264.402034   5311.748371   3042.146560   823.289726   \n",
       "..           ...           ...           ...           ...          ...   \n",
       "163  5632.111612  13052.873768  33256.701991  18058.628689  5482.338019   \n",
       "164  3881.099056  10506.889078  28213.065690  15709.712814  4451.614922   \n",
       "165  2650.130905   7756.237717  22599.452866  12739.017214  3438.028237   \n",
       "166  1708.101388   5273.201435  15775.263871   9468.200727  2457.632547   \n",
       "167   979.644477   3568.745768   9933.802338   6479.700047  1741.749364   \n",
       "\n",
       "             2510         3000          4510         5510          6000  \n",
       "0     3370.409520   267.786028   1479.324829  1336.970471   1889.604904  \n",
       "1     2900.235893   237.712564   1239.254616  1097.984215   1575.578501  \n",
       "2     2623.984029   238.233660   1212.557481   990.063390   1429.250499  \n",
       "3     2309.397095   246.475241   1279.705455   953.161538   1415.914875  \n",
       "4     2318.923533   280.901976   1578.093550  1181.196390   2459.799594  \n",
       "..            ...          ...           ...          ...           ...  \n",
       "163  16973.545140  2855.807821  13466.158350  8605.197958  15150.040284  \n",
       "164  13205.617957  1917.205749   9414.607451  6428.157459  10995.838902  \n",
       "165  10220.189805  1314.046429   6650.817474  4821.256399   7842.087456  \n",
       "166   7479.922532   861.730292   4503.500760  3402.110653   5208.806618  \n",
       "167   5065.378998   499.849024   2728.745200  2216.997636   3224.551945  \n",
       "\n",
       "[168 rows x 36 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions = torch.cat(predictions, dim=0)\n",
    "submission_file_path = os.path.join(DATASET_PATH, 'sample_submission.csv')\n",
    "submission_table = pd.read_csv(submission_file_path)\n",
    "submission_table.iloc[:,1:]= train_dataset.scaler.unscale(predictions)[0]\n",
    "submission_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5c058-1467-4151-89d3-12c7e7d81aef",
   "metadata": {},
   "source": [
    "#### 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9cb6202-ef85-4b2c-b2c5-7a7ffe567593",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_table.to_csv('prediction_20.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38229c3-3bd7-4afa-87b2-fd7c88ecb6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
